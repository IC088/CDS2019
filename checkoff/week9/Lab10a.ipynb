{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab10a",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KDMeojC4XA9R"
      },
      "source": [
        "## Training a word2vec model from scratch\n",
        "\n",
        "-- Prof. Dorien Herremans\n",
        "\n",
        "We will start by training a word2vec model from scratch using the gensim library. You will need to ensure that you have gensim installed, and a file decompressor to load our dataset. \n",
        "\n",
        "Note: these models may take a while to train. Be sure to switch the runtime of  Google Colab to us a TPU or GPU hardware accellerator (in the menu at the top). \n",
        "\n",
        "Let's start by installing some libraries that we will use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDkvVQRXu8rj",
        "colab_type": "code",
        "outputId": "9c9cde21-ced4-4df5-cfe1-403d2a74ccc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "!pip install gensim\n",
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.4)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.10.14)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.13.14)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qiQjAboxcBN",
        "colab_type": "text"
      },
      "source": [
        "Now we can import these libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5wZmGH0XHYP",
        "colab": {}
      },
      "source": [
        "# imports needed \n",
        "import gensim\n",
        "import wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02-XoaTBYuY1",
        "colab_type": "text"
      },
      "source": [
        "We will train our model using a very small dataset for demonstrative purposes. Note that for a real data science project you should train on a much larger dataset. \n",
        "\n",
        "We will use the complete works of Shakespeare. You can find the file at https://dorienherremans.com/drop/CDS/CNNs/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tg9GZzYt4nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the dataset\n",
        "wget.download(\"https://dorienherremans.com/drop/CDS/CNNs/shakespeare.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKHwTYP3YpJZ",
        "colab_type": "text"
      },
      "source": [
        "Let's read the input file and convert each line into a list of words (tokenizing). Do do this, we create a function read_input which is called in the penultimate line below: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8J2jQ1JYpJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_input(input_file):\n",
        "    print(\"reading file...\")\n",
        "    with open (input_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "    # do some pre-processing and return a (tokenized) list\n",
        "    # of words for each review text\n",
        "    # you can print the output here to understand\n",
        "    # the preprocessing (tokenizing)\n",
        "            yield gensim.utils.simple_preprocess(line)\n",
        "# each review item new becomes a series of words\n",
        "# this is a list of lists\n",
        "# point to the location on your filesystem\n",
        "data_file = 'shakespeare.txt'\n",
        "documents = list (read_input(data_file))\n",
        "print(\"Done reading data file\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39mZIXvRKRDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnDX2S9eX1Ub",
        "colab_type": "text"
      },
      "source": [
        "Now let's train the word2vec model using our document variable (which is a list of word lists). Note that you can specify a number of hyperparameters below:\n",
        "* min_count removes all words that occur less then min_count\n",
        "* window: window size in the skip-gram\n",
        "* workers: how many threads to use\n",
        "* size: number of dimension of your new word embedding vector (typically 100-200). Smaller datasets require a smaller number\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VJWmrzCRZm9Z",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec (documents, size=150, window=5, min_count=2, workers=4)\n",
        "model.train(documents,total_examples=len(documents),epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hKwagKCYpJ7",
        "colab_type": "text"
      },
      "source": [
        "That's it! Now you've trained the model! \n",
        "\n",
        "Now let's explore some properties of our new word space. You can get the words most close (read:  most similar) to a given word. Remember, the only texts the model has seen is shakespeare!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTGeHhiaZzIU",
        "colab": {}
      },
      "source": [
        "w1 = \"king\"\n",
        "model.wv.most_similar (positive=w1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GuJhHUOve1km",
        "colab": {}
      },
      "source": [
        "# look up top 6 words similar to 'smile'\n",
        "w1 = \"smile\"\n",
        "model.wv.most_similar (positive=w1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iZu3GGf9e41V",
        "colab": {}
      },
      "source": [
        "# look up top 6 words similar to 'france'\n",
        "w1 = [\"france\"]\n",
        "model.wv.most_similar (positive=w1,topn=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VlDIzGHZe9yN",
        "colab": {}
      },
      "source": [
        "# look up top 6 words similar to 'sword'\n",
        "w1 = [\"sword\"]\n",
        "model.wv.most_similar (positive=w1,topn=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvKKzFHbfAnr",
        "colab": {}
      },
      "source": [
        "# get everything related to stuff on the royalty and not related to farmer\n",
        "w1 = [\"king\",'queen','prince']\n",
        "w2 = ['farmer']\n",
        "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkQZ1KkiYpK_",
        "colab_type": "text"
      },
      "source": [
        "Explore the similarity (e.g. distance) between two words. Does it make sense?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3nFv31TxfBMX",
        "colab": {}
      },
      "source": [
        "# similarity between two similar words\n",
        "model.wv.similarity(w1=\"pretty\",w2=\"beautiful\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PXLoSftYpLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# similarity between two opposing words\n",
        "model.wv.similarity(w1=\"king\",w2=\"farmer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpftKgByYpLT",
        "colab_type": "text"
      },
      "source": [
        "Try some other combinations :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw7Aabx4YpLU",
        "colab_type": "text"
      },
      "source": [
        "We can even use it to perform more 'smart' assigments: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8eO7ra-efKYq",
        "colab": {}
      },
      "source": [
        "# Which one is the odd one out in this list?\n",
        "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll3RsRN2YpL3",
        "colab_type": "text"
      },
      "source": [
        "If you are interested in plotting the words in a multidimensional space, you can actually get the vector coordinates of each word: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEOzUfKcYpL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.wv['france']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_U5DgEPYpMI",
        "colab_type": "text"
      },
      "source": [
        "## Bonus: visualising our model in t-SNE: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g6g7s-MYpMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def tsne_plot(model):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "\n",
        "    labels = []\n",
        "    tokens = []\n",
        "    \n",
        "    count = 0\n",
        "    for word in model.wv.vocab:\n",
        "        # to speed up the process, let's limit to the first 100 elements\n",
        "        if count < 100:\n",
        "            # TODO get the labels\n",
        "            count = count+1\n",
        "\n",
        "    # set the t-sne values\n",
        "    # TODO fit the t-sne model\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()\n",
        "    \n",
        "tsne_plot(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsRf6k4wYpMd",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "* https://radimrehurek.com/gensim/models/word2vec.html\n",
        "* https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
        "* https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec\n",
        "* https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
      ]
    }
  ]
}